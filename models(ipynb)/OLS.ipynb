{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear method makes use of the log backward return (log price difference) to predict foward return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "1) Ridge regression: on 30 features\n",
    "2) PC regression: pca on 30 features then perform ols\n",
    "\n",
    "Feature: 10 stocks, each with 3 backward return (say, 3min, 7min, 10min, see correlation to decide)\n",
    "\n",
    "Response: 10 stocks' 30min forward return. \n",
    "\n",
    "Groups: [1,4,5,6,8],[0,2,3,7,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "log_pr = pd.read_pickle(\"../data/log_price.df\")\n",
    "volu = pd.read_pickle(\"../data/volume_usd.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wide_format(df):\n",
    "#     df_= df.reset_index(level=['stock']).sort_index()\n",
    "#     df_ = df_.pivot(columns ='stock')\n",
    "#     df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "#     return df_\n",
    "\n",
    "\n",
    "# def get_feature_train(log_pr, volu, x_begin_idx, x_end_idx, y_begin_idx, \n",
    "#                         grp_idx=None, rm_outlier=False, print_cor=True):\n",
    "#     \"\"\"\n",
    "#     Input:\n",
    "#     log_pr (pdSeries): train set\n",
    "#     volu (pdSeries): train set\n",
    "#     x_begin_idx (pdIndex): to truncate the NaNs\n",
    "#     grp_idx (dict): key is group idx, value is list of stock idx\n",
    "\n",
    "#     Returns:\n",
    "#     feature_dict (dict): key is group idx, value is a tuple of feature matrix and response\n",
    "#     \"\"\"\n",
    "\n",
    "#     log_pr_df = log_pr.reset_index().melt(id_vars=['timestamp'])\n",
    "#     log_pr_df.columns = ['timestamp', 'stock', 'log_pr']\n",
    "#     log_pr_df = log_pr_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "#     volu_df = volu.reset_index().melt(id_vars=['timestamp'])\n",
    "#     volu_df.columns = ['timestamp', 'stock', 'volu']\n",
    "#     volu_df = volu_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "#     features = pd.DataFrame(index=log_pr_df.index)\n",
    "#     # log_pr feature\n",
    "#     # for i in [10, 20, 30]:\n",
    "#     #     features['log_pr_{}'.format(i)] = -log_pr_df.groupby(level='stock').log_pr.diff(i)\n",
    "#     features['log_pr_{}'.format(30)] = -log_pr_df.groupby(level='stock').log_pr.diff(30)\n",
    "\n",
    "#     std_10 = lambda x: x.rolling(10).std()\n",
    "#     features['log_pr_std_10'] = log_pr_df.groupby(level='stock').log_pr.apply(std_10)\n",
    "\n",
    "#     # volume feature\n",
    "#     log_fn = lambda x: np.log(x+1)\n",
    "#     features['log_volu'] = volu_df.groupby(level='stock').volu.apply(log_fn)\n",
    "\n",
    "#     # stdised volume in 2 hours backward rolling windows\n",
    "#     zscore_fn = lambda x: (x - x.rolling(window=240, min_periods=20).mean()) / x.rolling(window=240, min_periods=20).std()\n",
    "#     features['volu_z_score'] = volu_df.groupby(level='stock').volu.apply(zscore_fn)\n",
    "\n",
    "#     # feature_dropped = features.iloc[x_begin_idx:x_end_idx]\n",
    "#     response = log_pr.diff(30)\n",
    "\n",
    "#     if grp_idx is not None:\n",
    "#         feature_dict = {}\n",
    "#         for key, idx_lis in grp_idx.items():\n",
    "#             feature_df_dropped = wide_format(features.loc[pd.IndexSlice[:,idx_lis],:])\n",
    "#             # transform back to wide format\n",
    "#             feature_dict[key] = (feature_df_dropped.iloc[x_begin_idx:x_end_idx], \n",
    "#                                             response[idx_lis].iloc[y_begin_idx:])\n",
    "#         return feature_dict\n",
    "#     else:\n",
    "#         # transform back to wide format\n",
    "#         feature_df_dropped = wide_format(features).iloc[x_begin_idx:x_end_idx]\n",
    "#         # feature_df_dropped = feature_df[x_begin_idx:x_end_idx]\n",
    "    \n",
    "#         if print_cor:\n",
    "#             for i in range(10):\n",
    "#                 feature_train_0 = features.xs(i, level='stock')\n",
    "#                 print(feature_train_0.corrwith(response[i]))\n",
    "\n",
    "#         return feature_df_dropped, response.iloc[y_begin_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format(df):\n",
    "    df_= df.reset_index(level=['stock']).sort_index()\n",
    "    df_ = df_.pivot(columns ='stock')\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def rsi(close_delta, periods=20, ema=True):\n",
    "    \"\"\"\n",
    "    Returns a pd.Series with the relative strength index.\n",
    "    \"\"\"\n",
    "    close_delta = close_delta.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "\t    # Use exponential moving average\n",
    "        ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "def get_feature_train(log_pr, volu, x_begin_idx, x_end_idx, y_begin_idx, \n",
    "                        grp_idx=None, rm_outlier=False, print_cor=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    log_pr (pdSeries): train set\n",
    "    volu (pdSeries): train set\n",
    "    x_begin_idx (pdIndex): to truncate the NaNs\n",
    "    grp_idx (dict): key is group idx, value is list of stock idx\n",
    "\n",
    "    Returns:\n",
    "    feature_dict (dict): key is group idx, value is a tuple of feature matrix and response\n",
    "    \"\"\"\n",
    "\n",
    "    log_pr_df = log_pr.reset_index().melt(id_vars=['timestamp'])\n",
    "    log_pr_df.columns = ['timestamp', 'stock', 'log_pr']\n",
    "    log_pr_df = log_pr_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    volu_df = volu.reset_index().melt(id_vars=['timestamp'])\n",
    "    volu_df.columns = ['timestamp', 'stock', 'volu']\n",
    "    volu_df = volu_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    features = pd.DataFrame(index=log_pr_df.index)\n",
    "\n",
    "    # log_pr feature\n",
    "    for i in [20, 30]:\n",
    "        features['log_pr_{}'.format(i)] = -log_pr_df.groupby(level='stock').log_pr.diff(i)\n",
    "\n",
    "    # # EMA\n",
    "    # ema = lambda x: x.ewm(span=i).mean()\n",
    "    # for i in [10, 30, 50]:\n",
    "    #     features['pr_ema_{}'.format(i)] = log_pr_df.groupby(level='stock').log_pr.apply(ema)\n",
    "\n",
    "    # # MA\n",
    "    # for i in [10, 30, 50]:\n",
    "    #     ma = lambda x: x.rolling(i).mean()\n",
    "    #     features['pr_ma_{}'.format(i)] = log_pr_df.groupby(level='stock').apply(ma)\n",
    "\n",
    "    k_period = 40\n",
    "    d_period = 3\n",
    "    ma_max = lambda x: x.rolling(k_period).max()\n",
    "    ma_min = lambda x: x.rolling(k_period).min()\n",
    "    mad = lambda x: x.rolling(d_period).mean()\n",
    "    msd = lambda x: x.rolling(d_period).sum()\n",
    "\n",
    "    features['pr_min_40'] = log_pr_df.groupby(level='stock').log_pr.apply(ma_min)\n",
    "    features['pr_max_40'] = log_pr_df.groupby(level='stock').log_pr.apply(ma_max)\n",
    "\n",
    "    features['pr_so_40'] = (log_pr_df.log_pr - features['pr_min_40'])*100 / (features['pr_max_40'] - features['pr_min_40'])\n",
    "    features['pr_so_40d3'] = features.groupby(level='stock').pr_so_40.apply(mad)\n",
    "\n",
    "    # STD of log price\n",
    "    for i in [30]:\n",
    "        std = lambda x: x.rolling(i).std()\n",
    "        features['log_pr_std_{}'.format(i)] = log_pr_df.groupby(level='stock').log_pr.apply(std)\n",
    "\n",
    "    # RSI\n",
    "    # features['rsi_20'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi)\n",
    "    features['rsi_30'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi, periods=30)\n",
    "    # features['rsi_50'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi, periods=50)\n",
    "\n",
    "    # volume feature\n",
    "    log_fn = lambda x: np.log(x+1)\n",
    "    features['log_volu'] = volu_df.groupby(level='stock').volu.apply(log_fn)\n",
    "\n",
    "    # stdised volume in 2 hours backward rolling windows\n",
    "    zscore_fn = lambda x: (x - x.rolling(window=30, min_periods=20).mean()) / x.rolling(window=240, min_periods=20).std()\n",
    "    features['volu_z_score'] = volu_df.groupby(level='stock').volu.apply(zscore_fn)\n",
    "\n",
    "    # # Chaikin's money flow\n",
    "    # features['mf_40'] = volu_df.volu * ((2*log_pr_df.log_pr - features['pr_min_40'])\n",
    "    #                             / (features['pr_max_40'] - features['pr_min_40']))\n",
    "    # features['mf_40_ma'] = (features.groupby(level='stock').mf_40.apply(msd) / \n",
    "    #                         volu_df.groupby(level='stock').volu.apply(msd))\n",
    "\n",
    "    # feature_dropped = features.iloc[x_begin_idx:x_end_idx]\n",
    "    response = log_pr.diff(30)\n",
    "    # print(features.shape)\n",
    "    # print(feature_dropped.shape)\n",
    "    # print(response_dropped.shape)\n",
    "\n",
    "    if grp_idx is not None:\n",
    "        feature_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            feature_df_dropped = wide_format(features.loc[pd.IndexSlice[:,idx_lis],:])\n",
    "            # transform back to wide format\n",
    "            feature_dict[key] = (feature_df_dropped.iloc[x_begin_idx:x_end_idx], \n",
    "                                            response[idx_lis].iloc[y_begin_idx:])\n",
    "        return feature_dict\n",
    "    else:\n",
    "        # transform back to wide format\n",
    "        feature_df_dropped = wide_format(features).iloc[x_begin_idx:x_end_idx]\n",
    "        # feature_df_dropped = feature_df[x_begin_idx:x_end_idx]\n",
    "    \n",
    "        if print_cor:\n",
    "            for i in range(10):\n",
    "                \n",
    "                feature_train_0 = features.xs(i, level='stock').iloc[x_begin_idx:x_end_idx]\n",
    "                print(feature_train_0.corrwith(response[i]))\n",
    "                print(feature_train_0.isnull().sum())\n",
    "\n",
    "        return feature_df_dropped, response.iloc[y_begin_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_idx = {i: [i] for i in range(10)}\n",
    "\n",
    "x_begin_idx = 30\n",
    "x_end_idx = -30\n",
    "y_begin_idx = 60\n",
    "\n",
    "train_split_t = log_pr.index[-87841]\n",
    "#vali_split_t = log_pr.index[-44641]\n",
    "\n",
    "train_feature_dict = get_feature_train(log_pr[:train_split_t], volu[:train_split_t], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx, print_cor=False)\n",
    "\n",
    "vali_feature_dict = get_feature_train(log_pr[train_split_t:], volu[train_split_t:], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx,print_cor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain0, ytrain0 = train_feature_dict[0]\n",
    "xvali0, yvali0 = vali_feature_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg0 = OLS(ytrain0.values, xtrain0.values).fit()\n",
    "# reg0.summary()\n",
    "# mean_squared_error(ytrain0, reg0.predict())\n",
    "# mean_squared_error(yvali0, reg0.predict(xvali0))\n",
    "# np.corrcoef(yvali0.squeeze(), reg0.predict(xvali0).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264841, 1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat((ytrain0,yvali0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\royul\\Documents\\GitHub\\multivariate-time-series-prediction\\models(ipynb)\\step_reg.py:12: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  new_pval = pd.Series(index=excluded)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The indices for endog and exog are not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\royul\\Documents\\GitHub\\multivariate-time-series-prediction\\models(ipynb)\\OLS.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/OLS.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstep_reg\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/OLS.ipynb#ch0000047?line=1'>2</a>\u001b[0m step_reg\u001b[39m.\u001b[39;49mforward_regression(xtrain0,ytrain0,\u001b[39m0.05\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\royul\\Documents\\GitHub\\multivariate-time-series-prediction\\models(ipynb)\\step_reg.py:14\u001b[0m, in \u001b[0;36mforward_regression\u001b[1;34m(X, y, threshold_in, verbose)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/step_reg.py?line=11'>12</a>\u001b[0m new_pval \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(index\u001b[39m=\u001b[39mexcluded)\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/step_reg.py?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m new_column \u001b[39min\u001b[39;00m excluded:\n\u001b[1;32m---> <a href='file:///c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/step_reg.py?line=13'>14</a>\u001b[0m     model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mOLS(y, sm\u001b[39m.\u001b[39;49madd_constant(pd\u001b[39m.\u001b[39;49mDataFrame(X[included\u001b[39m+\u001b[39;49m[new_column]])))\u001b[39m.\u001b[39mfit()\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/step_reg.py?line=14'>15</a>\u001b[0m     new_pval[new_column] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpvalues[new_column]\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/Documents/GitHub/multivariate-time-series-prediction/models%28ipynb%29/step_reg.py?line=15'>16</a>\u001b[0m best_pval \u001b[39m=\u001b[39m new_pval\u001b[39m.\u001b[39mmin()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:890\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=886'>887</a>\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=887'>888</a>\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mAn exception will be raised in the next version.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=888'>889</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=889'>890</a>\u001b[0m \u001b[39msuper\u001b[39;49m(OLS, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(endog, exog, missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=890'>891</a>\u001b[0m                           hasconst\u001b[39m=\u001b[39;49mhasconst, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=891'>892</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_keys:\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=892'>893</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_keys\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:717\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=714'>715</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=715'>716</a>\u001b[0m     weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=716'>717</a>\u001b[0m \u001b[39msuper\u001b[39;49m(WLS, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(endog, exog, missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=717'>718</a>\u001b[0m                           weights\u001b[39m=\u001b[39;49mweights, hasconst\u001b[39m=\u001b[39;49mhasconst, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=718'>719</a>\u001b[0m nobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=719'>720</a>\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:191\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=189'>190</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endog, exog, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=190'>191</a>\u001b[0m     \u001b[39msuper\u001b[39;49m(RegressionModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(endog, exog, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/regression/linear_model.py?line=191'>192</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_attr\u001b[39m.\u001b[39mextend([\u001b[39m'\u001b[39m\u001b[39mpinv_wexog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwendog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwexog\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\base\\model.py:267\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=265'>266</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, endog, exog\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=266'>267</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(endog, exog, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=267'>268</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitialize()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\base\\model.py:92\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=89'>90</a>\u001b[0m missing \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmissing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=90'>91</a>\u001b[0m hasconst \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mhasconst\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=92'>93</a>\u001b[0m                               \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=93'>94</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_constant \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mk_constant\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=94'>95</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexog \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexog\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\base\\model.py:132\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=130'>131</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_handle_data\u001b[39m(\u001b[39mself\u001b[39m, endog, exog, missing, hasconst, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=131'>132</a>\u001b[0m     data \u001b[39m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=132'>133</a>\u001b[0m     \u001b[39m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/model.py?line=133'>134</a>\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\base\\data.py:673\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=669'>670</a>\u001b[0m     exog \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(exog)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=671'>672</a>\u001b[0m klass \u001b[39m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=672'>673</a>\u001b[0m \u001b[39mreturn\u001b[39;00m klass(endog, exog\u001b[39m=\u001b[39;49mexog, missing\u001b[39m=\u001b[39;49mmissing, hasconst\u001b[39m=\u001b[39;49mhasconst,\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=673'>674</a>\u001b[0m              \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\base\\data.py:87\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_constant \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=85'>86</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_constant(hasconst)\n\u001b[1;32m---> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=86'>87</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_integrity()\n\u001b[0;32m     <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=87'>88</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\statsmodels\\base\\data.py:531\u001b[0m, in \u001b[0;36mPandasData._check_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=526'>527</a>\u001b[0m \u001b[39m# exog can be None and we could be upcasting one or the other\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m (exog \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=528'>529</a>\u001b[0m         (\u001b[39mhasattr\u001b[39m(endog, \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(exog, \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=529'>530</a>\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_endog\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_exog\u001b[39m.\u001b[39mindex)):\n\u001b[1;32m--> <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=530'>531</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe indices for endog and exog are not aligned\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/royul/miniconda3/lib/site-packages/statsmodels/base/data.py?line=531'>532</a>\u001b[0m \u001b[39msuper\u001b[39m(PandasData, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_check_integrity()\n",
      "\u001b[1;31mValueError\u001b[0m: The indices for endog and exog are not aligned"
     ]
    }
   ],
   "source": [
    "import step_reg\n",
    "step_reg.forward_regression(xtrain0,ytrain0,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1330510.5344174337"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg0.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function for the ten stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OLS(feature_dict):\n",
    "    mod_dict = {}\n",
    "    for i, (X, y) in feature_dict.items():\n",
    "        mod_dict[i] = OLS(y.values, X.values).fit()\n",
    "\n",
    "    return mod_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict = train_OLS(train_feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models.pckl\", \"wb\") as f:\n",
    "    for model in mod_dict.values():\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format_test(df):\n",
    "    df_= df.reset_index()\n",
    "    df_ = df_.pivot(columns ='index').apply(lambda s: s.dropna().reset_index(drop=True))\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def get_feature_test(log_pr, volu, grp_idx=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    log_pr (pdSeries): 1 day of log pr \n",
    "    volu (pdSeries): 1 day of volume\n",
    "\n",
    "    Output:\n",
    "    test data frame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=log_pr.columns)\n",
    "\n",
    "    # backward return\n",
    "    # print(-(log_pr.iloc[-1] - log_pr.iloc[-30]).values)\n",
    "    #for i in [10, 20, 30]:\n",
    "    #    features['log_pr_{}'.format(i)] = -(log_pr.iloc[-1] - log_pr.iloc[-i]).values\n",
    "    features['log_pr_{}'.format(30)] = -(log_pr.iloc[-1] - log_pr.iloc[-30]).values\n",
    "    # backward rolling std\n",
    "    features['log_pr_std_10'] = log_pr.iloc[-10:].std(0).values\n",
    "    \n",
    "    # volume features\n",
    "    features['log_volu'] = np.log(volu.iloc[-1].values + 1)\n",
    "    features['volu_z_score'] = ((volu.iloc[-1] - volu.iloc[-240:].mean())/volu.iloc[-240:].std()).values\n",
    "\n",
    "    if grp_idx is None:\n",
    "        return wide_format_test(features)\n",
    "    else:\n",
    "        df_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            df_dict[key] = wide_format_test(features.loc[idx_lis])\n",
    "        return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = mod_dict #{i: pickle.load(open('../model/ridge{}.sav'.format(i), 'rb')) for i in range(2)}\n",
    "\n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    grp_idx = {i: [i] for i in range(10)}\n",
    "    x = get_feature_test(A, B, grp_idx=grp_idx)\n",
    "    pred_dict = {i: model.predict(x[i]) for i, model in model_dict.items()}\n",
    "    \n",
    "    out = np.zeros(10)\n",
    "    for keys, idx in grp_idx.items():\n",
    "        out[idx] = pred_dict.get(keys)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_r_hat_tune(A, B):\n",
    "#     # grp_idx = {0:[1,5,6,8], 1:[0,2,3,4,7,9]}\n",
    "#     x = get_feature_test(A, B)\n",
    "#     return rr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tune(log_pr_test, volu_test):\n",
    "\n",
    "    t0 = time.time()\n",
    "    dt = datetime.timedelta(days=1)\n",
    "\n",
    "    r_fwd = (log_pr_test.shift(-30) - log_pr_test).iloc[1440::10]\n",
    "    # r_fwd = return_true.iloc[1440::10]\n",
    "    # r_fwd.index = log_pr_test.index[1440::10]\n",
    "    r_hat = pd.DataFrame(index=log_pr_test.index[1440::10], columns=log_pr_test.columns, dtype=np.float64)\n",
    "\n",
    "    for t in log_pr_test.index[1440::10]: # compute the predictions every 10 minutes\n",
    "        # inputs 1 day of log price and volume\n",
    "        r_hat.loc[t, :] = get_r_hat(log_pr_test.loc[(t - dt):t], volu_test.loc[(t - dt):t])\n",
    "    t_used = time.time() - t0\n",
    "    print(\"Time used: \", t_used)\n",
    "\n",
    "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final 3 rows are NaNs. \n",
    "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
    "    return r_fwd.corrwith(r_hat), np.corrcoef(r_fwd_all, r_hat_all)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_test = log_pr\n",
    "volu_test = volu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tune(log_pr_test, volu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_train_vali = log_pr[:train_split_t]\n",
    "volu_train_vali = volu[:train_split_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tune(log_pr_train_vali, volu_train_vali)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcbeb1c3c4ccee2109855bb42e1bb012102ee5721b972abedda41e225a004887"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
