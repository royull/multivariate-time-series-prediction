{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear method makes use of the log backward return (log price difference) to predict foward return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "1) Ridge regression: on 30 features\n",
    "2) PC regression: pca on 30 features then perform ols\n",
    "\n",
    "Feature: 10 stocks, each with 3 backward return (say, 3min, 7min, 10min, see correlation to decide)\n",
    "\n",
    "Response: 10 stocks' 30min forward return. \n",
    "\n",
    "Groups: [1,4,5,6,8],[0,2,3,7,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "log_pr = pd.read_pickle(\"../data/log_price.df\")\n",
    "volu = pd.read_pickle(\"../data/volume_usd.df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format(df):\n",
    "    df_= df.reset_index(level=['stock']).sort_index()\n",
    "    df_ = df_.pivot(columns ='stock')\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def rsi(close_delta, periods=20, ema=True):\n",
    "    \"\"\"\n",
    "    Returns a pd.Series with the relative strength index.\n",
    "    \"\"\"\n",
    "    close_delta = close_delta.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "\t    # Use exponential moving average\n",
    "        ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "def get_feature_train(log_pr, volu, x_begin_idx, x_end_idx, y_begin_idx, \n",
    "                        grp_idx=None, rm_outlier=False, print_cor=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    log_pr (pdSeries): train set\n",
    "    volu (pdSeries): train set\n",
    "    x_begin_idx (pdIndex): to truncate the NaNs\n",
    "    grp_idx (dict): key is group idx, value is list of stock idx\n",
    "\n",
    "    Returns:\n",
    "    feature_dict (dict): key is group idx, value is a tuple of feature matrix and response\n",
    "    \"\"\"\n",
    "\n",
    "    log_pr_df = log_pr.reset_index().melt(id_vars=['timestamp'])\n",
    "    log_pr_df.columns = ['timestamp', 'stock', 'log_pr']\n",
    "    log_pr_df = log_pr_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    volu_df = volu.reset_index().melt(id_vars=['timestamp'])\n",
    "    volu_df.columns = ['timestamp', 'stock', 'volu']\n",
    "    volu_df = volu_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    features = pd.DataFrame(index=log_pr_df.index)\n",
    "\n",
    "    # log_pr feature\n",
    "    for i in [20, 30]:\n",
    "        features['log_pr_{}'.format(i)] = -log_pr_df.groupby(level='stock').log_pr.diff(i)\n",
    "\n",
    "    # # EMA\n",
    "    # ema = lambda x: x.ewm(span=i).mean()\n",
    "    # for i in [10, 30, 50]:\n",
    "    #     features['pr_ema_{}'.format(i)] = log_pr_df.groupby(level='stock').log_pr.apply(ema)\n",
    "\n",
    "    # # MA\n",
    "    # for i in [10, 30, 50]:\n",
    "    #     ma = lambda x: x.rolling(i).mean()\n",
    "    #     features['pr_ma_{}'.format(i)] = log_pr_df.groupby(level='stock').apply(ma)\n",
    "\n",
    "    k_period = 40\n",
    "    d_period = 3\n",
    "    ma_max = lambda x: x.rolling(k_period).max()\n",
    "    ma_min = lambda x: x.rolling(k_period).min()\n",
    "    mad = lambda x: x.rolling(d_period).mean()\n",
    "    msd = lambda x: x.rolling(d_period).sum()\n",
    "\n",
    "    features['pr_min_40'] = log_pr_df.groupby(level='stock').log_pr.apply(ma_min)\n",
    "    features['pr_max_40'] = log_pr_df.groupby(level='stock').log_pr.apply(ma_max)\n",
    "\n",
    "    features['pr_so_40'] = (log_pr_df.log_pr - features['pr_min_40'])*100 / (features['pr_max_40'] - features['pr_min_40'])\n",
    "    features['pr_so_40d3'] = features.groupby(level='stock').pr_so_40.apply(mad)\n",
    "\n",
    "    # STD of log price\n",
    "    for i in [30]:\n",
    "        std = lambda x: x.rolling(i).std()\n",
    "        features['log_pr_std_{}'.format(i)] = log_pr_df.groupby(level='stock').log_pr.apply(std)\n",
    "\n",
    "    # RSI\n",
    "    # features['rsi_20'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi)\n",
    "    features['rsi_30'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi, periods=30)\n",
    "    # features['rsi_50'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi, periods=50)\n",
    "\n",
    "    # volume feature\n",
    "    log_fn = lambda x: np.log(x+1)\n",
    "    features['log_volu'] = volu_df.groupby(level='stock').volu.apply(log_fn)\n",
    "\n",
    "    # stdised volume in 2 hours backward rolling windows\n",
    "    zscore_fn = lambda x: (x - x.rolling(window=30, min_periods=20).mean()) / x.rolling(window=240, min_periods=20).std()\n",
    "    features['volu_z_score'] = volu_df.groupby(level='stock').volu.apply(zscore_fn)\n",
    "\n",
    "    # # Chaikin's money flow\n",
    "    # features['mf_40'] = volu_df.volu * ((2*log_pr_df.log_pr - features['pr_min_40'])\n",
    "    #                             / (features['pr_max_40'] - features['pr_min_40']))\n",
    "    # features['mf_40_ma'] = (features.groupby(level='stock').mf_40.apply(msd) / \n",
    "    #                         volu_df.groupby(level='stock').volu.apply(msd))\n",
    "\n",
    "    # feature_dropped = features.iloc[x_begin_idx:x_end_idx]\n",
    "    response = log_pr.diff(30)\n",
    "    # print(features.shape)\n",
    "    # print(feature_dropped.shape)\n",
    "    # print(response_dropped.shape)\n",
    "\n",
    "    if grp_idx is not None:\n",
    "        feature_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            feature_df_dropped = wide_format(features.loc[pd.IndexSlice[:,idx_lis],:])\n",
    "            # transform back to wide format\n",
    "            feature_dict[key] = (feature_df_dropped.iloc[x_begin_idx:x_end_idx], \n",
    "                                            response[idx_lis].iloc[y_begin_idx:])\n",
    "        return feature_dict\n",
    "    else:\n",
    "        # transform back to wide format\n",
    "        feature_df_dropped = wide_format(features).iloc[x_begin_idx:x_end_idx]\n",
    "        # feature_df_dropped = feature_df[x_begin_idx:x_end_idx]\n",
    "    \n",
    "        if print_cor:\n",
    "            for i in range(10):\n",
    "                \n",
    "                feature_train_0 = features.xs(i, level='stock').iloc[x_begin_idx:x_end_idx]\n",
    "                print(feature_train_0.corrwith(response[i]))\n",
    "                print(feature_train_0.isnull().sum())\n",
    "\n",
    "        return feature_df_dropped, response.iloc[y_begin_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_idx = {i: [i] for i in range(10)}\n",
    "\n",
    "x_begin_idx = 41\n",
    "x_end_idx = -30\n",
    "y_begin_idx = 71\n",
    "\n",
    "train_split_t = log_pr.index[-87841]\n",
    "#vali_split_t = log_pr.index[-44641]\n",
    "\n",
    "train_feature_dict = get_feature_train(log_pr[:train_split_t], volu[:train_split_t], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx, print_cor=False)\n",
    "\n",
    "test_feature_dict = get_feature_train(log_pr[train_split_t:], volu[train_split_t:], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx,print_cor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import itertools as it\n",
    "\n",
    "def forward_regression(X, y):\n",
    "    '''\n",
    "    Input\n",
    "    X,y: training matrix (without intercept)\n",
    "    Return\n",
    "    model: ols model fitted with intercept on the selected features\n",
    "    feature_selected: selected features\n",
    "    '''\n",
    "    initial_list = []\n",
    "    included = list(initial_list)\n",
    "    feature_num = 5#len(X.columns)\n",
    "    best_bics = pd.Series(index={i for i in range(feature_num)})\n",
    "    best_features = list(it.repeat([],feature_num))\n",
    "    for k in range(feature_num):\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_bic = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_bic[new_column] = model.bic\n",
    "        best_bic = new_bic.min()\n",
    "        best_bics[k] = best_bic\n",
    "        best_feature = new_bic.idxmin()\n",
    "        included.append(best_feature)\n",
    "        best_features[k] = included.copy()\n",
    "    feature_selected = best_features[best_bics.idxmin()]\n",
    "    model = sm.OLS(y,X[feature_selected]).fit() #sm.add_constant(pd.DataFrame(X[feature_selected]))).fit()\n",
    "    return model,feature_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>2</td>        <th>  R-squared (uncentered):</th>       <td>   0.004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th>  <td>   0.004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>           <td>   148.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 18 Apr 2022</td> <th>  Prob (F-statistic):</th>           <td>1.17e-157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:30:58</td>     <th>  Log-Likelihood:    </th>          <td>5.9219e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>177049</td>      <th>  AIC:               </th>          <td>-1.184e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>177044</td>      <th>  BIC:               </th>          <td>-1.184e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>               <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>               <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsi_30_2</th>        <td>-4.871e-05</td> <td> 2.55e-06</td> <td>  -19.084</td> <td> 0.000</td> <td>-5.37e-05</td> <td>-4.37e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_pr_std_30_2</th> <td>    0.0608</td> <td>    0.009</td> <td>    6.888</td> <td> 0.000</td> <td>    0.044</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>volu_z_score_2</th>  <td>    0.0001</td> <td> 2.09e-05</td> <td>    5.227</td> <td> 0.000</td> <td> 6.83e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pr_min_40_2</th>     <td>   -0.0010</td> <td> 5.56e-05</td> <td>  -17.935</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_volu_2</th>      <td>    0.0003</td> <td> 1.31e-05</td> <td>   19.992</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>93540.287</td> <th>  Durbin-Watson:     </th>  <td>   0.074</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>2311386.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.036</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>20.226</td>   <th>  Cond. No.          </th>  <td>2.24e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 2.24e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      2   R-squared (uncentered):                   0.004\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.004\n",
       "Method:                 Least Squares   F-statistic:                              148.3\n",
       "Date:                Mon, 18 Apr 2022   Prob (F-statistic):                   1.17e-157\n",
       "Time:                        16:30:58   Log-Likelihood:                      5.9219e+05\n",
       "No. Observations:              177049   AIC:                                 -1.184e+06\n",
       "Df Residuals:                  177044   BIC:                                 -1.184e+06\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "rsi_30_2        -4.871e-05   2.55e-06    -19.084      0.000   -5.37e-05   -4.37e-05\n",
       "log_pr_std_30_2     0.0608      0.009      6.888      0.000       0.044       0.078\n",
       "volu_z_score_2      0.0001   2.09e-05      5.227      0.000    6.83e-05       0.000\n",
       "pr_min_40_2        -0.0010   5.56e-05    -17.935      0.000      -0.001      -0.001\n",
       "log_volu_2          0.0003   1.31e-05     19.992      0.000       0.000       0.000\n",
       "==============================================================================\n",
       "Omnibus:                    93540.287   Durbin-Watson:                   0.074\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2311386.103\n",
       "Skew:                           2.036   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.226   Cond. No.                     2.24e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 2.24e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain0, ytrain0 = train_feature_dict[2]\n",
    "#xtest0, ytest0 = test_feature_dict[0]\n",
    "ytrain0 = ytrain0.set_index(xtrain0.index)\n",
    "#ytest0 = ytest0.set_index(xtest0.index)\n",
    "reg0,feature0 = forward_regression(xtrain0,ytrain0)\n",
    "reg0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rsi_30_2', 'log_pr_std_30_2', 'volu_z_score_2', 'pr_min_40_2', 'log_volu_2']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_pr_20_0', 'log_pr_30_0', 'pr_min_40_0', 'pr_max_40_0',\n",
       "       'pr_so_40_0', 'pr_so_40d3_0', 'log_pr_std_30_0', 'rsi_30_0',\n",
       "       'log_volu_0', 'volu_z_score_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OLS(feature_dict):\n",
    "    mod_dict = {}\n",
    "    for i, (X, y) in feature_dict.items():\n",
    "        mod_dict[i] = OLS(y.values, X.values).fit()\n",
    "\n",
    "    return mod_dict\n",
    "\n",
    "mod_dict = train_OLS(train_feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function Subseted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subOLS(feature_dict):\n",
    "    mod_dict = {}\n",
    "    feature_dict = {}\n",
    "    for i, (X, y) in feature_dict.items():\n",
    "        print(X)\n",
    "        model, feature = forward_regression(y,X)\n",
    "        mod_dict[i] = model\n",
    "        feature_dict[i] = feature\n",
    "    return mod_dict,feature_dict\n",
    "\n",
    "mod_dict, fea_dict = train_subOLS(train_feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models.pckl\", \"wb\") as f:\n",
    "    for model in mod_dict.values():\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format_test(df):\n",
    "    df_= df.reset_index()\n",
    "    df_ = df_.pivot(columns ='index').apply(lambda s: s.dropna().reset_index(drop=True))\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def get_feature_test(log_pr, volu, grp_idx=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    log_pr (pdSeries): 1 day of log pr \n",
    "    volu (pdSeries): 1 day of volume\n",
    "\n",
    "    Output:\n",
    "    test data frame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=log_pr.columns)\n",
    "\n",
    "    # backward return\n",
    "    # print(-(log_pr.iloc[-1] - log_pr.iloc[-30]).values)\n",
    "    for i in [10, 20, 30]:\n",
    "        features['log_pr_{}'.format(i)] = -(log_pr.iloc[-1] - log_pr.iloc[-i]).values\n",
    "    # backward rolling std\n",
    "    features['log_pr_std_10'] = log_pr.iloc[-10:].std(0).values\n",
    "    \n",
    "    # volume features\n",
    "    features['log_volu'] = np.log(volu.iloc[-1].values + 1)\n",
    "    features['volu_z_score'] = ((volu.iloc[-1] - volu.iloc[-240:].mean())/volu.iloc[-240:].std()).values\n",
    "\n",
    "    if grp_idx is None:\n",
    "        return wide_format_test(features)\n",
    "    else:\n",
    "        df_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            df_dict[key] = wide_format_test(features.loc[idx_lis])\n",
    "        return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = mod_dict #{i: pickle.load(open('../model/ridge{}.sav'.format(i), 'rb')) for i in range(2)}\n",
    "\n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    grp_idx = {i: [i] for i in range(10)}\n",
    "    x = get_feature_test(A, B, grp_idx=grp_idx)\n",
    "    pred_dict = {i: model.predict(x[i]) for i, model in model_dict.items()}\n",
    "    \n",
    "    out = np.zeros(10)\n",
    "    for keys, idx in grp_idx.items():\n",
    "        out[idx] = pred_dict.get(keys)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tune(log_pr_test, volu_test):\n",
    "\n",
    "    t0 = time.time()\n",
    "    dt = datetime.timedelta(days=1)\n",
    "\n",
    "    r_fwd = (log_pr_test.shift(-30) - log_pr_test).iloc[1440::10]\n",
    "    # r_fwd = return_true.iloc[1440::10]\n",
    "    # r_fwd.index = log_pr_test.index[1440::10]\n",
    "    r_hat = pd.DataFrame(index=log_pr_test.index[1440::10], columns=log_pr_test.columns, dtype=np.float64)\n",
    "\n",
    "    for t in log_pr_test.index[1440::10]: # compute the predictions every 10 minutes\n",
    "        # inputs 1 day of log price and volume\n",
    "        r_hat.loc[t, :] = get_r_hat(log_pr_test.loc[(t - dt):t], volu_test.loc[(t - dt):t])\n",
    "    t_used = time.time() - t0\n",
    "    print(\"Time used: \", t_used)\n",
    "\n",
    "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final 3 rows are NaNs. \n",
    "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
    "    return r_fwd.corrwith(r_hat), np.corrcoef(r_fwd_all, r_hat_all)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_test = log_pr[:train_split_t]\n",
    "volu_test = volu[:train_split_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tune(log_pr_test, volu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_test = log_pr\n",
    "volu_test = volu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tune(log_pr_test, volu_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcbeb1c3c4ccee2109855bb42e1bb012102ee5721b972abedda41e225a004887"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
