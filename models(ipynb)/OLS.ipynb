{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear method makes use of the log backward return (log price difference) to predict foward return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "1) Ridge regression: on 30 features\n",
    "2) PC regression: pca on 30 features then perform ols\n",
    "\n",
    "Feature: 10 stocks, each with 3 backward return (say, 3min, 7min, 10min, see correlation to decide)\n",
    "\n",
    "Response: 10 stocks' 30min forward return. \n",
    "\n",
    "Groups: [1,4,5,6,8],[0,2,3,7,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "log_pr = pd.read_pickle(\"../data/log_price.df\")\n",
    "volu = pd.read_pickle(\"../data/volume_usd.df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi(close_delta, periods=20, ema=True):\n",
    "    \"\"\"\n",
    "    Returns a pd.Series with the relative strength index.\n",
    "    \"\"\"\n",
    "    close_delta = close_delta.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    if ema == True:\n",
    "\t    # Use exponential moving average\n",
    "        ma_up = up.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "        ma_down = down.ewm(com = periods - 1, adjust=True, min_periods = periods).mean()\n",
    "    else:\n",
    "        # Use simple moving average\n",
    "        ma_up = up.rolling(window = periods, adjust=False).mean()\n",
    "        ma_down = down.rolling(window = periods, adjust=False).mean()\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def wide_format(df):\n",
    "    df_= df.reset_index(level=['stock']).sort_index()\n",
    "    df_ = df_.pivot(columns ='stock')\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "def get_feature_train(log_pr, volu, x_begin_idx, x_end_idx, y_begin_idx, \n",
    "                        grp_idx=None, rm_outlier=False, print_cor=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    log_pr (pdSeries): train set\n",
    "    volu (pdSeries): train set\n",
    "    x_begin_idx (pdIndex): to truncate the NaNs\n",
    "    grp_idx (dict): key is group idx, value is list of stock idx\n",
    "\n",
    "    Returns:\n",
    "    feature_dict (dict): key is group idx, value is a tuple of feature matrix and response\n",
    "    \"\"\"\n",
    "\n",
    "    log_pr_df = log_pr.reset_index().melt(id_vars=['timestamp'])\n",
    "    log_pr_df.columns = ['timestamp', 'stock', 'log_pr']\n",
    "    log_pr_df = log_pr_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    volu_df = volu.reset_index().melt(id_vars=['timestamp'])\n",
    "    volu_df.columns = ['timestamp', 'stock', 'volu']\n",
    "    volu_df = volu_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    features = pd.DataFrame(index=log_pr_df.index)\n",
    "    # features['trend'] = np.ones(log_pr_df.shape[0])\n",
    "\n",
    "    # log_pr feature\n",
    "    for i in [30]:\n",
    "        features['log_pr_{}'.format(i)] = -log_pr_df.groupby(level='stock').log_pr.diff(i)\n",
    "\n",
    "    k_period = 40\n",
    "    d_period = 3\n",
    "    ma_max = lambda x: x.rolling(k_period).max()\n",
    "    ma_min = lambda x: x.rolling(k_period).min()\n",
    "    mad = lambda x: x.rolling(d_period).mean()\n",
    "    # msd = lambda x: x.rolling(d_period).sum()\n",
    "\n",
    "    features['pr_min_40'] = log_pr_df.groupby(level='stock').log_pr.apply(ma_min)\n",
    "    features['pr_max_40'] = log_pr_df.groupby(level='stock').log_pr.apply(ma_max)\n",
    "\n",
    "    features['pr_so_40'] = (log_pr_df.log_pr - features['pr_min_40'])*100 / (features['pr_max_40'] - features['pr_min_40'])\n",
    "    features['pr_so_40d3'] = features.groupby(level='stock').pr_so_40.apply(mad)\n",
    "\n",
    "    # STD of log price\n",
    "    for i in [10,30]:\n",
    "        std = lambda x: x.rolling(i).std()\n",
    "        features['log_pr_std_{}'.format(i)] = log_pr_df.groupby(level='stock').log_pr.apply(std)\n",
    "\n",
    "    # RSI\n",
    "    # features['rsi_20'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi)\n",
    "    features['rsi_30'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi, periods=30)\n",
    "    # features['rsi_50'] = log_pr_df.groupby(level='stock').log_pr.apply(rsi, periods=50)\n",
    "\n",
    "    # volume feature\n",
    "    log_fn = lambda x: np.log(x+1)\n",
    "    features['log_volu'] = volu_df.groupby(level='stock').volu.apply(log_fn)\n",
    "\n",
    "    # stdised volume in 2 hours backward rolling windows\n",
    "    zscore_fn = lambda x: (x - x.rolling(window=240, min_periods=20).mean()) / x.rolling(window=240, min_periods=20).std()\n",
    "    features['volu_z_score'] = volu_df.groupby(level='stock').volu.apply(zscore_fn)\n",
    "\n",
    "\n",
    "    # drop min, max features\n",
    "    features = features.drop(columns=['pr_min_40', 'pr_max_40', 'pr_so_40'])\n",
    "\n",
    "    response = log_pr.diff(30)\n",
    "\n",
    "    if grp_idx is not None:\n",
    "        feature_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            feature_df_dropped = wide_format(features.loc[pd.IndexSlice[:,idx_lis],:])\n",
    "            # transform back to wide format\n",
    "            feature_dict[key] = (feature_df_dropped.iloc[x_begin_idx:x_end_idx], \n",
    "                                            response[idx_lis].iloc[y_begin_idx:])\n",
    "        return feature_dict\n",
    "    else:\n",
    "        # transform back to wide format\n",
    "        feature_df_dropped = wide_format(features).iloc[x_begin_idx:x_end_idx]\n",
    "        # feature_df_dropped = feature_df[x_begin_idx:x_end_idx]\n",
    "    \n",
    "        if print_cor:\n",
    "            for i in range(10):\n",
    "                \n",
    "                feature_train_0 = features.xs(i, level='stock').iloc[x_begin_idx:x_end_idx]\n",
    "                print(feature_train_0.corrwith(response[i]))\n",
    "                print(feature_train_0.isnull().sum())\n",
    "\n",
    "        return feature_df_dropped, response.iloc[y_begin_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_idx = {i: [i] for i in range(10)}\n",
    "\n",
    "x_begin_idx = 41\n",
    "x_end_idx = -30\n",
    "y_begin_idx = 71\n",
    "\n",
    "train_split_t = log_pr.index[-87841]\n",
    "#vali_split_t = log_pr.index[-44641]\n",
    "\n",
    "train_feature_dict = get_feature_train(log_pr[:train_split_t], volu[:train_split_t], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx, print_cor=False)\n",
    "\n",
    "test_feature_dict = get_feature_train(log_pr[train_split_t:], volu[train_split_t:], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx,print_cor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with AIC (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain0, ytrain0 = train_feature_dict[2]\n",
    "ytrain0 = ytrain0.set_index(xtrain0.index)\n",
    "#reg0,feature0 = forward_regression(xtrain0,ytrain0)\n",
    "#reg0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OLS(feature_dict):\n",
    "    mod_dict = {}\n",
    "    for i, (X, y) in feature_dict.items():\n",
    "        mod_dict[i] = OLS(y.values, X.values).fit()\n",
    "    return mod_dict\n",
    "\n",
    "mod_dict = train_OLS(train_feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function Subseted LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import itertools as it\n",
    "\n",
    "def forward_regression(X, y):\n",
    "    '''\n",
    "    Input\n",
    "    X,y: training matrix (without intercept)\n",
    "    Return\n",
    "    model: ols model fitted with intercept on the selected features\n",
    "    feature_selected: selected features\n",
    "    '''\n",
    "    initial_list = []\n",
    "    included = list(initial_list)\n",
    "    feature_num = len(X.columns)\n",
    "    best_bics = pd.Series(index={i for i in range(feature_num)})\n",
    "    best_features = list(it.repeat([],feature_num))\n",
    "    for k in range(feature_num):\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_bic = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_bic[new_column] = model.bic\n",
    "        best_bic = new_bic.min()\n",
    "        best_bics[k] = best_bic\n",
    "        best_feature = new_bic.idxmin()\n",
    "        included.append(best_feature)\n",
    "        best_features[k] = included.copy()\n",
    "    feature_selected = best_features[best_bics.idxmin()]\n",
    "    model = sm.OLS(y,sm.add_constant(pd.DataFrame(X[feature_selected]))).fit()\n",
    "    return model,feature_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "feature_dict = {}\n",
    "for i, (X, y) in train_feature_dict.items():\n",
    "    y = y.set_index(X.index)\n",
    "    model, feature = forward_regression(X,y)\n",
    "    model_dict[i] = model\n",
    "    feature_dict[i] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['rsi_30_0'],\n",
       " 1: ['log_pr_30_1', 'log_pr_std_30_1', 'rsi_30_1', 'log_pr_std_10_1'],\n",
       " 2: ['volu_z_score_2',\n",
       "  'rsi_30_2',\n",
       "  'log_pr_std_10_2',\n",
       "  'pr_so_40d3_2',\n",
       "  'log_volu_2'],\n",
       " 3: ['log_pr_30_3', 'log_pr_std_30_3', 'rsi_30_3', 'log_volu_3'],\n",
       " 4: ['log_pr_30_4',\n",
       "  'log_pr_std_30_4',\n",
       "  'log_volu_4',\n",
       "  'pr_so_40d3_4',\n",
       "  'log_pr_std_10_4'],\n",
       " 5: ['log_pr_std_10_5', 'log_pr_30_5', 'volu_z_score_5'],\n",
       " 6: ['rsi_30_6',\n",
       "  'log_volu_6',\n",
       "  'log_pr_30_6',\n",
       "  'volu_z_score_6',\n",
       "  'pr_so_40d3_6',\n",
       "  'log_pr_std_10_6'],\n",
       " 7: ['pr_so_40d3_7',\n",
       "  'log_pr_std_30_7',\n",
       "  'log_pr_std_10_7',\n",
       "  'volu_z_score_7',\n",
       "  'log_volu_7',\n",
       "  'log_pr_30_7',\n",
       "  'rsi_30_7'],\n",
       " 8: ['log_pr_30_8',\n",
       "  'log_pr_std_10_8',\n",
       "  'log_pr_std_30_8',\n",
       "  'rsi_30_8',\n",
       "  'volu_z_score_8'],\n",
       " 9: ['log_pr_std_10_9', 'log_pr_std_30_9', 'log_pr_30_9', 'rsi_30_9']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models.pckl\", \"wb\") as f:\n",
    "    for model in mod_dict.values():\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetted models\n",
    "import pickle\n",
    "with open(\"models_subsetls.pckl\", \"wb\") as f:\n",
    "    for model in mod_dict.values():\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS\n",
    "def wide_format_test(df):\n",
    "    df_= df.reset_index()\n",
    "    df_ = df_.pivot(columns ='index').apply(lambda s: s.dropna().reset_index(drop=True))\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def get_feature_test(log_pr, volu, grp_idx=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    log_pr (pdSeries): 1 day of log pr \n",
    "    volu (pdSeries): 1 day of volume\n",
    "\n",
    "    Output:\n",
    "    test data frame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=log_pr.columns)\n",
    "\n",
    "    # backward return\n",
    "    # print(-(log_pr.iloc[-1] - log_pr.iloc[-30]).values)\n",
    "    for i in [10, 20, 30]:\n",
    "        features['log_pr_{}'.format(i)] = -(log_pr.iloc[-1] - log_pr.iloc[-i]).values\n",
    "    # backward rolling std\n",
    "    features['log_pr_std_10'] = log_pr.iloc[-10:].std(0).values\n",
    "    \n",
    "    # volume features\n",
    "    features['log_volu'] = np.log(volu.iloc[-1].values + 1)\n",
    "    features['volu_z_score'] = ((volu.iloc[-1] - volu.iloc[-240:].mean())/volu.iloc[-240:].std()).values\n",
    "\n",
    "    if grp_idx is None:\n",
    "        return wide_format_test(features)\n",
    "    else:\n",
    "        df_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            df_dict[key] = wide_format_test(features.loc[idx_lis])\n",
    "        return df_dict\n",
    "\n",
    "model_dict = mod_dict #{i: pickle.load(open('../model/ridge{}.sav'.format(i), 'rb')) for i in range(2)}\n",
    "\n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    grp_idx = {i: [i] for i in range(10)}\n",
    "    x = get_feature_test(A, B, grp_idx=grp_idx)\n",
    "    pred_dict = {i: model.predict(x[i]) for i, model in model_dict.items()}\n",
    "    \n",
    "    out = np.zeros(10)\n",
    "    for keys, idx in grp_idx.items():\n",
    "        out[idx] = pred_dict.get(keys)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset LS help function\n",
    "def rsi_test(log_pr, periods=20):\n",
    "    \"\"\"\n",
    "    Returns a pd.Series with the relative strength index.\n",
    "    \"\"\"\n",
    "    close_delta = log_pr.diff()\n",
    "\n",
    "    # Make two series: one for lower closes and one for higher closes\n",
    "    up = close_delta.clip(lower=0)\n",
    "    down = -1 * close_delta.clip(upper=0)\n",
    "    \n",
    "    # Use exponential moving average\n",
    "    ma_up = up.ewm(com=periods-1, adjust=True, min_periods=periods).mean().iloc[-1]\n",
    "    ma_down = down.ewm(com=periods-1, adjust=True, min_periods=periods).mean().iloc[-1]\n",
    "        \n",
    "    rsi = ma_up / ma_down\n",
    "    rsi = 100 - (100/(1 + rsi))\n",
    "    return rsi\n",
    "\n",
    "def wide_format_test(df):\n",
    "    df_= df.reset_index()\n",
    "    df_ = df_.pivot(columns ='index').apply(lambda s: s.dropna().reset_index(drop=True))\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def get_feature_test(log_pr, volu, grp_idx=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    log_pr (pdSeries): 1 day of log pr \n",
    "    volu (pdSeries): 1 day of volume\n",
    "\n",
    "    Output:\n",
    "    test data frame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=log_pr.columns)\n",
    "    features['log_pr_30'] = -(log_pr.iloc[-1] - log_pr.iloc[-31]).values\n",
    "    \n",
    "    # Oscilator\n",
    "    k_period = 40\n",
    "    d_period = 3\n",
    "    pr_min_40 = log_pr.rolling(k_period).min().iloc[-d_period:].values\n",
    "    pr_max_40 = log_pr.rolling(k_period).max().iloc[-d_period:].values\n",
    "    pr_so_40 = (log_pr.iloc[-d_period:].values - pr_min_40)*100 / (pr_max_40 - pr_min_40)\n",
    "    features['pr_so_40d3']  = pr_so_40.mean(0)\n",
    "\n",
    "    # backward rolling std\n",
    "    features['log_pr_std_10'] = log_pr.iloc[-10:].std(0).values\n",
    "    features['log_pr_std_30'] = log_pr.iloc[-30:].std(0).values\n",
    "    \n",
    "    # RSI\n",
    "    features['rsi_30'] = log_pr.apply(rsi_test, periods=30)\n",
    "\n",
    "    # volume features\n",
    "    features['log_volu'] = np.log(volu.iloc[-1].values + 1)\n",
    "    features['volu_z_score'] = ((volu.iloc[-1] - volu.iloc[-240:].mean())/volu.iloc[-240:].std()).values\n",
    "\n",
    "    # print(volu.iloc[-240:].mean())\n",
    "\n",
    "    if grp_idx is None:\n",
    "        return wide_format_test(features)\n",
    "    else:\n",
    "        df_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            df_dict[key] = wide_format_test(features.loc[idx_lis])[feature_dict[key]]\n",
    "        return df_dict\n",
    "\n",
    "\n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    grp_idx = {i:[i] for i in range(10)}\n",
    "    x = get_feature_test(A, B, grp_idx=grp_idx)\n",
    "    pred_dict = {i: model.predict(np.insert(x[i].values,0,1.)) for i, model in model_dict.items()}\n",
    "    \n",
    "    out = np.zeros(10)\n",
    "    for keys, idx in grp_idx.items():\n",
    "        out[idx] = pred_dict.get(keys)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def evaluate_tune(log_pr_test, volu_test):\n",
    "\n",
    "    t0 = time.time()\n",
    "    dt = datetime.timedelta(days=1)\n",
    "\n",
    "    r_fwd = (log_pr_test.shift(-30) - log_pr_test).iloc[1440::10]\n",
    "    # r_fwd = return_true.iloc[1440::10]\n",
    "    # r_fwd.index = log_pr_test.index[1440::10]\n",
    "    r_hat = pd.DataFrame(index=log_pr_test.index[1440::10], columns=log_pr_test.columns, dtype=np.float64)\n",
    "\n",
    "    for t in log_pr_test.index[1440::10]: # compute the predictions every 10 minutes\n",
    "        # inputs 1 day of log price and volume\n",
    "        r_hat.loc[t, :] = get_r_hat(log_pr_test.loc[(t - dt):t], volu_test.loc[(t - dt):t])\n",
    "    t_used = time.time() - t0\n",
    "    print(\"Time used: \", t_used)\n",
    "\n",
    "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final 3 rows are NaNs. \n",
    "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
    "    return r_fwd.corrwith(r_hat), np.corrcoef(r_fwd_all, r_hat_all)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used:  790.7790622711182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0   -0.013158\n",
       " 1    0.021653\n",
       " 2    0.027461\n",
       " 3    0.001490\n",
       " 4    0.117057\n",
       " 5    0.026351\n",
       " 6    0.020502\n",
       " 7    0.010828\n",
       " 8    0.106681\n",
       " 9    0.049929\n",
       " dtype: float64,\n",
       " 0.03897242904209)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_pr_test = log_pr[train_split_t:]\n",
    "volu_test = volu[train_split_t:]\n",
    "evaluate_tune(log_pr_test, volu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_test = log_pr\n",
    "volu_test = volu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_tune(log_pr_test, volu_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcbeb1c3c4ccee2109855bb42e1bb012102ee5721b972abedda41e225a004887"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
