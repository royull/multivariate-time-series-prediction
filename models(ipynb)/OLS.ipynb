{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear method makes use of the log backward return (log price difference) to predict foward return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "1) Ridge regression: on 30 features\n",
    "2) PC regression: pca on 30 features then perform ols\n",
    "\n",
    "Feature: 10 stocks, each with 3 backward return (say, 3min, 7min, 10min, see correlation to decide)\n",
    "\n",
    "Response: 10 stocks' 30min forward return. \n",
    "\n",
    "Groups: [1,3,5,9],[2,4,7],[0],[3],[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "log_pr = pd.read_pickle(\"../data/log_price.df\")\n",
    "volu = pd.read_pickle(\"../data/volume_usd.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format(df):\n",
    "    df_= df.reset_index(level=['stock']).sort_index()\n",
    "    df_ = df_.pivot(columns ='stock')\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "def get_feature_train(log_pr, volu, x_begin_idx, x_end_idx, y_begin_idx, \n",
    "                        grp_idx=None, rm_outlier=False, print_cor=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    log_pr (pdSeries): train set\n",
    "    volu (pdSeries): train set\n",
    "    x_begin_idx (pdIndex): to truncate the NaNs\n",
    "    grp_idx (dict): key is group idx, value is list of stock idx\n",
    "\n",
    "    Returns:\n",
    "    feature_dict (dict): key is group idx, value is a tuple of feature matrix and response\n",
    "    \"\"\"\n",
    "\n",
    "    log_pr_df = log_pr.reset_index().melt(id_vars=['timestamp'])\n",
    "    log_pr_df.columns = ['timestamp', 'stock', 'log_pr']\n",
    "    log_pr_df = log_pr_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    volu_df = volu.reset_index().melt(id_vars=['timestamp'])\n",
    "    volu_df.columns = ['timestamp', 'stock', 'volu']\n",
    "    volu_df = volu_df.set_index(['timestamp', 'stock']).sort_index()\n",
    "\n",
    "    features = pd.DataFrame(index=log_pr_df.index)\n",
    "    # log_pr feature\n",
    "    for i in [10, 20, 30]:\n",
    "        features['log_pr_{}'.format(i)] = -log_pr_df.groupby(level='stock').log_pr.diff(i)\n",
    "\n",
    "    std_10 = lambda x: x.rolling(10).std()\n",
    "    features['log_pr_std_10'] = log_pr_df.groupby(level='stock').log_pr.apply(std_10)\n",
    "\n",
    "    # volume feature\n",
    "    log_fn = lambda x: np.log(x+1)\n",
    "    features['log_volu'] = volu_df.groupby(level='stock').volu.apply(log_fn)\n",
    "\n",
    "    # stdised volume in 2 hours backward rolling windows\n",
    "    zscore_fn = lambda x: (x - x.rolling(window=240, min_periods=20).mean()) / x.rolling(window=240, min_periods=20).std()\n",
    "    features['volu_z_score'] = volu_df.groupby(level='stock').volu.apply(zscore_fn)\n",
    "\n",
    "    # feature_dropped = features.iloc[x_begin_idx:x_end_idx]\n",
    "    response = log_pr.diff(30)\n",
    "\n",
    "    if grp_idx is not None:\n",
    "        feature_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            feature_df_dropped = wide_format(features.loc[pd.IndexSlice[:,idx_lis],:])\n",
    "            # transform back to wide format\n",
    "            feature_dict[key] = (feature_df_dropped.iloc[x_begin_idx:x_end_idx], \n",
    "                                            response[idx_lis].iloc[y_begin_idx:])\n",
    "        return feature_dict\n",
    "    else:\n",
    "        # transform back to wide format\n",
    "        feature_df_dropped = wide_format(features).iloc[x_begin_idx:x_end_idx]\n",
    "        # feature_df_dropped = feature_df[x_begin_idx:x_end_idx]\n",
    "    \n",
    "        if print_cor:\n",
    "            for i in range(10):\n",
    "                feature_train_0 = features.xs(i, level='stock')\n",
    "                print(feature_train_0.corrwith(response[i]))\n",
    "\n",
    "        return feature_df_dropped, response.iloc[y_begin_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_idx = {i: [i] for i in range(10)}\n",
    "\n",
    "x_begin_idx = 30\n",
    "x_end_idx = -30\n",
    "y_begin_idx = 60\n",
    "\n",
    "train_split_t = log_pr.index[-87841]\n",
    "vali_split_t = log_pr.index[-44641]\n",
    "\n",
    "train_feature_dict = get_feature_train(log_pr[:train_split_t], volu[:train_split_t], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx, print_cor=False)\n",
    "\n",
    "vali_feature_dict = get_feature_train(log_pr[train_split_t:], volu[train_split_t:], x_begin_idx, x_end_idx, y_begin_idx,\n",
    "                                        grp_idx=grp_idx,print_cor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Ridge Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain0, ytrain0 = train_feature_dict[0]\n",
    "xvali0, yvali0 = vali_feature_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain3, ytrain3 = train_feature_dict[3]\n",
    "xvali3, yvali3 = vali_feature_dict[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit OLS with current features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg0 = OLS(ytrain0.values, xtrain0.values).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg3 = OLS(ytrain3.values, xtrain3.values).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>       <td>   0.002</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th>  <td>   0.002</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>           <td>   47.58</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 18 Apr 2022</td> <th>  Prob (F-statistic):</th>           <td>1.17e-58</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:29:15</td>     <th>  Log-Likelihood:    </th>          <td>6.6508e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>177060</td>      <th>  AIC:               </th>          <td>-1.330e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>177054</td>      <th>  BIC:               </th>          <td>-1.330e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>               <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>               <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.0029</td> <td>    0.006</td> <td>    0.518</td> <td> 0.604</td> <td>   -0.008</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.0063</td> <td>    0.006</td> <td>    1.116</td> <td> 0.264</td> <td>   -0.005</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    0.0360</td> <td>    0.004</td> <td>    8.861</td> <td> 0.000</td> <td>    0.028</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>    0.0544</td> <td>    0.016</td> <td>    3.404</td> <td> 0.001</td> <td>    0.023</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>-8.123e-06</td> <td> 1.82e-06</td> <td>   -4.470</td> <td> 0.000</td> <td>-1.17e-05</td> <td>-4.56e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th> <td> 3.569e-06</td> <td> 1.27e-05</td> <td>    0.281</td> <td> 0.778</td> <td>-2.13e-05</td> <td> 2.84e-05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>86762.629</td> <th>  Durbin-Watson:     </th>  <td>   0.077</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>2607255.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.769</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>21.463</td>   <th>  Cond. No.          </th>  <td>1.49e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 1.49e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.002\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.002\n",
       "Method:                 Least Squares   F-statistic:                              47.58\n",
       "Date:                Mon, 18 Apr 2022   Prob (F-statistic):                    1.17e-58\n",
       "Time:                        09:29:15   Log-Likelihood:                      6.6508e+05\n",
       "No. Observations:              177060   AIC:                                 -1.330e+06\n",
       "Df Residuals:                  177054   BIC:                                 -1.330e+06\n",
       "Df Model:                           6                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0029      0.006      0.518      0.604      -0.008       0.014\n",
       "x2             0.0063      0.006      1.116      0.264      -0.005       0.017\n",
       "x3             0.0360      0.004      8.861      0.000       0.028       0.044\n",
       "x4             0.0544      0.016      3.404      0.001       0.023       0.086\n",
       "x5         -8.123e-06   1.82e-06     -4.470      0.000   -1.17e-05   -4.56e-06\n",
       "x6          3.569e-06   1.27e-05      0.281      0.778   -2.13e-05    2.84e-05\n",
       "==============================================================================\n",
       "Omnibus:                    86762.629   Durbin-Watson:                   0.077\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2607255.704\n",
       "Skew:                           1.769   Prob(JB):                         0.00\n",
       "Kurtosis:                      21.463   Cond. No.                     1.49e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 1.49e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>       <td>   0.004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th>  <td>   0.004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>           <td>   124.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 18 Apr 2022</td> <th>  Prob (F-statistic):</th>           <td>1.35e-157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:29:15</td>     <th>  Log-Likelihood:    </th>          <td>6.8386e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>177060</td>      <th>  AIC:               </th>          <td>-1.368e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>177054</td>      <th>  BIC:               </th>          <td>-1.368e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>               <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>               <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>    0.0460</td> <td>    0.005</td> <td>    8.436</td> <td> 0.000</td> <td>    0.035</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>    0.0233</td> <td>    0.005</td> <td>    4.298</td> <td> 0.000</td> <td>    0.013</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>    0.0300</td> <td>    0.004</td> <td>    7.550</td> <td> 0.000</td> <td>    0.022</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td>    0.1576</td> <td>    0.016</td> <td>    9.959</td> <td> 0.000</td> <td>    0.127</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>-9.002e-06</td> <td>  1.6e-06</td> <td>   -5.627</td> <td> 0.000</td> <td>-1.21e-05</td> <td>-5.87e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th> <td> 8.337e-06</td> <td> 1.14e-05</td> <td>    0.730</td> <td> 0.465</td> <td> -1.4e-05</td> <td> 3.07e-05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>93860.980</td> <th>  Durbin-Watson:     </th>  <td>   0.076</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>2989866.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.965</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>22.744</td>   <th>  Cond. No.          </th>  <td>1.68e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 1.68e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.004\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.004\n",
       "Method:                 Least Squares   F-statistic:                              124.4\n",
       "Date:                Mon, 18 Apr 2022   Prob (F-statistic):                   1.35e-157\n",
       "Time:                        09:29:15   Log-Likelihood:                      6.8386e+05\n",
       "No. Observations:              177060   AIC:                                 -1.368e+06\n",
       "Df Residuals:                  177054   BIC:                                 -1.368e+06\n",
       "Df Model:                           6                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0460      0.005      8.436      0.000       0.035       0.057\n",
       "x2             0.0233      0.005      4.298      0.000       0.013       0.034\n",
       "x3             0.0300      0.004      7.550      0.000       0.022       0.038\n",
       "x4             0.1576      0.016      9.959      0.000       0.127       0.189\n",
       "x5         -9.002e-06    1.6e-06     -5.627      0.000   -1.21e-05   -5.87e-06\n",
       "x6          8.337e-06   1.14e-05      0.730      0.465    -1.4e-05    3.07e-05\n",
       "==============================================================================\n",
       "Omnibus:                    93860.980   Durbin-Watson:                   0.076\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2989866.044\n",
       "Skew:                           1.965   Prob(JB):                         0.00\n",
       "Kurtosis:                      22.744   Cond. No.                     1.68e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 1.68e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.198004176691234e-05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ytrain0, reg0.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.58681160398662e-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ytrain3, reg3.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.055614327929686e-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(yvali0, reg0.predict(xvali0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.969906945327399e-05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(yvali3, reg3.predict(xvali3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00225744],\n",
       "       [0.00225744, 1.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(yvali3.squeeze(), reg3.predict(xvali3).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.00256128],\n",
       "       [-0.00256128,  1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(yvali0.squeeze(), reg0.predict(xvali0).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_OLS(feature_dict):\n",
    "    mod_dict = {}\n",
    "    for i, (X, y) in feature_dict.items():\n",
    "        mod_dict[i] = OLS(y, X).fit()\n",
    "\n",
    "    return mod_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict = train_OLS(train_feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"models.pckl\", \"wb\") as f:\n",
    "    for model in mod_dict.values():\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_format_test(df):\n",
    "    df_= df.reset_index()\n",
    "    df_ = df_.pivot(columns ='index').apply(lambda s: s.dropna().reset_index(drop=True))\n",
    "    df_.columns = df_.columns.get_level_values(0) + '_' +  [str(x) for x in df_.columns.get_level_values(1)]\n",
    "\n",
    "    return df_\n",
    "\n",
    "def get_feature_test(log_pr, volu, grp_idx=None):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    log_pr (pdSeries): 1 day of log pr \n",
    "    volu (pdSeries): 1 day of volume\n",
    "\n",
    "    Output:\n",
    "    test data frame\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=log_pr.columns)\n",
    "\n",
    "    # backward return\n",
    "    # print(-(log_pr.iloc[-1] - log_pr.iloc[-30]).values)\n",
    "    for i in [10, 20, 30]:\n",
    "        features['log_pr_{}'.format(i)] = -(log_pr.iloc[-1] - log_pr.iloc[-i]).values\n",
    "    # backward rolling std\n",
    "    features['log_pr_std_10'] = log_pr.iloc[-10:].std(0).values\n",
    "    \n",
    "    # volume features\n",
    "    features['log_volu'] = np.log(volu.iloc[-1].values + 1)\n",
    "    features['volu_z_score'] = ((volu.iloc[-1] - volu.iloc[-240:].mean())/volu.iloc[-240:].std()).values\n",
    "\n",
    "    if grp_idx is None:\n",
    "        return wide_format_test(features)\n",
    "    else:\n",
    "        df_dict = {}\n",
    "        for key, idx_lis in grp_idx.items():\n",
    "            df_dict[key] = wide_format_test(features.loc[idx_lis])\n",
    "        return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {i: pickle.load(open('../model/ridge{}.sav'.format(i), 'rb')) for i in range(2)}\n",
    "\n",
    "def get_r_hat(A, B): \n",
    "    \"\"\"\n",
    "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
    "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
    "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
    "    \"\"\"\n",
    "    grp_idx = {0:[1,5,6,8], 1:[0,2,3,4,7,9]}\n",
    "    x = get_feature_test(A, B, grp_idx=grp_idx)\n",
    "    pred_dict = {i: model.predict(x[i]) for i, model in model_dict.items()}\n",
    "    \n",
    "    out = np.zeros(10)\n",
    "    for keys, idx in grp_idx.items():\n",
    "        out[idx] = pred_dict.get(keys)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_r_hat_tune(A, B):\n",
    "#     # grp_idx = {0:[1,5,6,8], 1:[0,2,3,4,7,9]}\n",
    "#     x = get_feature_test(A, B)\n",
    "#     return rr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tune(log_pr_test, volu_test):\n",
    "\n",
    "    t0 = time.time()\n",
    "    dt = datetime.timedelta(days=1)\n",
    "\n",
    "    r_fwd = (log_pr_test.shift(-30) - log_pr_test).iloc[1440::10]\n",
    "    # r_fwd = return_true.iloc[1440::10]\n",
    "    # r_fwd.index = log_pr_test.index[1440::10]\n",
    "    r_hat = pd.DataFrame(index=log_pr_test.index[1440::10], columns=log_pr_test.columns, dtype=np.float64)\n",
    "\n",
    "    for t in log_pr_test.index[1440::10]: # compute the predictions every 10 minutes\n",
    "        # inputs 1 day of log price and volume\n",
    "        r_hat.loc[t, :] = get_r_hat(log_pr_test.loc[(t - dt):t], volu_test.loc[(t - dt):t])\n",
    "    t_used = time.time() - t0\n",
    "    print(\"Time used: \", t_used)\n",
    "\n",
    "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final 3 rows are NaNs. \n",
    "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
    "    return r_fwd.corrwith(r_hat), np.corrcoef(r_fwd_all, r_hat_all)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_test = log_pr\n",
    "volu_test = volu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used:  222.40197610855103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    0.056759\n",
       " 1    0.076502\n",
       " 2    0.053963\n",
       " 3    0.056852\n",
       " 4    0.126005\n",
       " 5    0.074002\n",
       " 6    0.079662\n",
       " 7    0.041148\n",
       " 8    0.110821\n",
       " 9    0.090038\n",
       " dtype: float64,\n",
       " 0.07260984465426996)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tune(log_pr_test, volu_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_train_vali = log_pr[:train_split_t]\n",
    "volu_train_vali = volu[:train_split_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used:  146.94605422019958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    0.043323\n",
       " 1    0.060267\n",
       " 2    0.048021\n",
       " 3    0.065409\n",
       " 4    0.124495\n",
       " 5    0.073912\n",
       " 6    0.090818\n",
       " 7    0.045182\n",
       " 8    0.106579\n",
       " 9    0.076561\n",
       " dtype: float64,\n",
       " 0.0659273964246186)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_tune(log_pr_train_vali, volu_train_vali)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcbeb1c3c4ccee2109855bb42e1bb012102ee5721b972abedda41e225a004887"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
